{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"applications/ai_tutor/","title":"AI Tutor","text":"<p>\ud83c\udfa5 Initial Demo Video</p> <p>Here is a preview of the tutor in action:</p> <p>Please note that this is an early demo, serving as a placeholder for now. A more polished version will be available soon.</p> <p>Stay tuned for further updates!</p>"},{"location":"code/dataloader/webcrawler/","title":"WebpageCrawler Class Documentation","text":""},{"location":"code/dataloader/webcrawler/#overview","title":"Overview","text":"<p>The <code>WebpageCrawler</code> class provides tools for crawling a given webpage and recursively exploring links to fetch all child pages under the same base URL. It validates the accessibility of links and categorizes them as either webpages or non-webpage resources based on their MIME type.</p>"},{"location":"code/dataloader/webcrawler/#class-reference","title":"Class Reference","text":""},{"location":"code/dataloader/webcrawler/#webpagecrawler","title":"WebpageCrawler","text":""},{"location":"code/dataloader/webcrawler/#attributes","title":"Attributes","text":"<ul> <li><code>dict_href_links</code> (dict): Stores and tracks discovered links during the crawl.</li> </ul>"},{"location":"code/dataloader/webcrawler/#methods","title":"Methods","text":""},{"location":"code/dataloader/webcrawler/#__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new instance of the <code>WebpageCrawler</code> class.</p> <p>Usage:</p> <pre><code>crawler = WebpageCrawler()\n</code></pre>"},{"location":"code/dataloader/webcrawler/#async-fetchsession-clientsession-url-str-str","title":"<code>async fetch(session: ClientSession, url: str) -&gt; str</code>","text":"<p>Asynchronously fetches the HTML content of the specified URL.</p> <p>Parameters:</p> <ul> <li><code>session</code> (ClientSession): The session used to make HTTP requests.</li> <li><code>url</code> (str): The URL to fetch.</li> </ul> <p>Returns:</p> <ul> <li>(str): The HTML content of the page.</li> </ul> <p>Raises:</p> <ul> <li><code>aiohttp.ClientError</code>: If the HTTP request fails.</li> </ul> <p>Usage:</p> <pre><code>try:\n    html_content = await crawler.fetch(session, url)\nexcept aiohttp.ClientError as e:\n    print(f\"Failed to fetch {url}: {e}\")\n</code></pre>"},{"location":"code/dataloader/webcrawler/#url_existsurl-str-bool","title":"<code>url_exists(url: str) -&gt; bool</code>","text":"<p>Checks if a given URL is accessible by performing a <code>HEAD</code> request.</p> <p>Parameters:</p> <ul> <li><code>url</code> (str): The URL to check.</li> </ul> <p>Returns:</p> <ul> <li>(bool): <code>True</code> if the URL is accessible (status code 200), otherwise <code>False</code>.</li> </ul> <p>Usage:</p> <pre><code>if crawler.url_exists(url):\n    print(\"URL exists\")\nelse:\n    print(\"URL does not exist\")\n</code></pre>"},{"location":"code/dataloader/webcrawler/#async-get_linkssession-clientsession-website_link-str-base_url-str-liststr","title":"<code>async get_links(session: ClientSession, website_link: str, base_url: str) -&gt; List[str]</code>","text":"<p>Extracts and normalizes valid links from the HTML content of a webpage.</p> <p>Parameters:</p> <ul> <li><code>session</code> (ClientSession): The session used for making HTTP requests.</li> <li><code>website_link</code> (str): The URL of the webpage to extract links from.</li> <li><code>base_url</code> (str): The base URL to filter out external links.</li> </ul> <p>Returns:</p> <ul> <li>(List[str]): A list of normalized URLs that are valid and fall under the base URL.</li> </ul> <p>Usage:</p> <pre><code>links = await crawler.get_links(session, website_link, base_url)\n</code></pre>"},{"location":"code/dataloader/webcrawler/#async-get_subpage_linkssession-clientsession-urls-liststr-base_url-str-liststr","title":"<code>async get_subpage_links(session: ClientSession, urls: List[str], base_url: str) -&gt; List[str]</code>","text":"<p>Asynchronously gathers links from multiple webpages.</p> <p>Parameters:</p> <ul> <li><code>session</code> (ClientSession): The session used for making HTTP requests.</li> <li><code>urls</code> (List[str]): A list of URLs to fetch links from.</li> <li><code>base_url</code> (str): The base URL to filter out external links.</li> </ul> <p>Returns:</p> <ul> <li>(List[str]): A combined list of all child URLs discovered from the provided list of URLs.</li> </ul> <p>Usage:</p> <pre><code>all_links = await crawler.get_subpage_links(session, urls, base_url)\n</code></pre>"},{"location":"code/dataloader/webcrawler/#async-get_all_pagesurl-str-base_url-str-liststr","title":"<code>async get_all_pages(url: str, base_url: str) -&gt; List[str]</code>","text":"<p>Recursively crawls a website to gather all valid URLs under the same base URL.</p> <p>Parameters:</p> <ul> <li><code>url</code> (str): The initial URL to start crawling from.</li> <li><code>base_url</code> (str): The base URL to filter out external links.</li> </ul> <p>Returns:</p> <ul> <li>(List[str]): A complete list of all URLs discovered under the base URL.</li> </ul> <p>Usage:</p> <pre><code>all_pages = await crawler.get_all_pages(url, base_url)\n</code></pre>"},{"location":"contributing/","title":"Index","text":"Info <pre><code>Please ensure formatting, linting, and security checks pass before submitting a pull request.\n</code></pre>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<p>The codebase is formatted using black</p> <p>To format the codebase, run the following command:</p> <pre><code>black .\n</code></pre> <p>Please ensure that the code is formatted before submitting a pull request.</p>"},{"location":"contributing/#linting","title":"Linting","text":"<p>The codebase is linted using flake8</p> <p>To view the linting errors, run the following command:</p> <pre><code>flake8 .\n</code></pre>"},{"location":"contributing/#security-and-vulnerabilities","title":"Security and Vulnerabilities","text":"<p>The codebase is scanned for security vulnerabilities using bandit</p> <p>To scan the codebase for security vulnerabilities, run the following command:</p> <pre><code>bandit -r .\n</code></pre>"},{"location":"getting%20started/run_locally/","title":"Run locally","text":""},{"location":"getting%20started/run_locally/#running-locally","title":"Running Locally","text":"<p>Clone the Repository <pre><code>git clone https://github.com/DL4DS/dl4ds_tutor\n</code></pre></p> <p>Put your data under the <code>storage/data</code> directory  - Add URLs in the <code>urls.txt</code> file.   - Add other PDF files in the <code>storage/data</code> directory.    </p> <p>To test Data Loading (Optional) <pre><code>cd code\npython -m modules.dataloader.data_loader\n</code></pre></p> <p>Create the Vector Database <pre><code>cd code\npython -m modules.vectorstore.store_manager\n</code></pre> - Note: You need to run the above command when you add new data to the <code>storage/data</code> directory, or if the <code>storage/data/urls.txt</code> file is updated.    </p> <p>Run the App <pre><code>cd code\nuvicorn app:app --port 7860  \n</code></pre></p>"},{"location":"getting%20started/setup/","title":"Setup","text":"<p>Terrier Tutor is built using Chainlit, a Python framework to build conversational chatbots powered by LLMs. </p>"},{"location":"getting%20started/setup/#initial-setup","title":"Initial Setup","text":"Important <pre><code>Do create your .env file and set the required environment variables as described in the sections below.\n</code></pre>"},{"location":"getting%20started/setup/#set-up-python-environment","title":"Set up Python Environment","text":"<p>Python Version: 3.11</p> <p>Create a Python virtual environment: </p> CondaVirtualenv <pre><code>conda create -n ai_tutor python=3.11\nconda activate ai_tutor\n</code></pre> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Install dependencies</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting%20started/setup/#code-formatting","title":"Code Formatting","text":"<p>The codebase is formatted using black, and if making changes to the codebase, ensure that the code is formatted before submitting a pull request. More instructions can be found in <code>docs/contribute.md</code>.</p>"},{"location":"getting%20started/setup/#google-oauth-20-client-id-and-secret","title":"Google OAuth 2.0 Client ID and Secret","text":"<p>To set up the Google OAuth 2.0 Client ID and Secret, follow these steps:</p> <ol> <li>Go to the Google Cloud Console.</li> <li>Create a new project or select an existing one.</li> <li>Navigate to the \"Credentials\" page.</li> <li>Click on \"Create Credentials\" and select \"OAuth 2.0 Client ID\".</li> <li>Configure the OAuth consent screen if you haven't already.</li> <li>Choose \"Web application\" as the application type.</li> <li>Configure the redirect URIs as needed.</li> <li>Copy the generated <code>Client ID</code> and <code>Client Secret</code>.</li> </ol> <p>Set the following in the .env file (if running locally) or in secrets (if running on Hugging Face Spaces):</p> <pre><code>OAUTH_GOOGLE_CLIENT_ID=&lt;your_client_id&gt;\nOAUTH_GOOGLE_CLIENT_SECRET=&lt;your_client_secret&gt;\n</code></pre>"},{"location":"getting%20started/setup/#literal-ai-api-key","title":"Literal AI API Key","text":"<p>To obtain the Literal AI API key:</p> <ol> <li>Sign up or log in to Literal AI.</li> <li>Navigate to the API Keys section under your account settings.</li> <li>Create a new API key if necessary and copy it.</li> </ol> <p>Set the following in the .env file (if running locally) or in secrets (if running on Hugging Face Spaces):</p> <pre><code>LITERAL_API_KEY_LOGGING=&lt;your_api_key&gt;\nLITERAL_API_URL=https://cloud.getliteral.ai\n</code></pre>"},{"location":"getting%20started/setup/#llamacloud-api-key","title":"LlamaCloud API Key","text":"<p>To obtain the LlamaCloud API Key:</p> <ol> <li>Go to LlamaCloud.</li> <li>Sign up or log in to your account.</li> <li>Navigate to the API section and generate a new API key if necessary.</li> </ol> <p>Set the following in the .env file (if running locally) or in secrets (if running on Hugging Face Spaces):</p> <pre><code>LLAMA_CLOUD_API_KEY=&lt;your_api_key&gt;\n</code></pre>"},{"location":"getting%20started/setup/#hugging-face-access-token","title":"Hugging Face Access Token","text":"<p>To obtain your Hugging Face access token:</p> <ol> <li>Go to Hugging Face settings.</li> <li>Log in or create an account.</li> <li>Generate a new token or use an existing one.</li> </ol> <p>Set the following in the .env file (if running locally) or in secrets (if running on Hugging Face Spaces):</p> <pre><code>HUGGINGFACE_TOKEN=&lt;your-huggingface-token&gt;\n</code></pre>"},{"location":"getting%20started/setup/#chainlit-authentication-secret","title":"Chainlit Authentication Secret","text":"<p>You must provide a JWT secret in the environment to use authentication. Run <code>chainlit create-secret</code> to generate one.</p> <pre><code>chainlit create-secret\n</code></pre> <p>Set the following in the .env file (if running locally) or in secrets (if running on Hugging Face Spaces):</p> <pre><code>CHAINLIT_AUTH_SECRET=&lt;your_jwt_secret&gt;\nCHAINLIT_URL=&lt;your_chainlit_url&gt; # Example: CHAINLIT_URL=http://localhost:8000\n</code></pre>"},{"location":"getting%20started/setup/#openai-api-key","title":"OpenAI API Key","text":"<p>Set the following in the .env file (if running locally) or in secrets (if running on Hugging Face Spaces):</p> <pre><code>OPENAI_API_KEY=&lt;your_openai_api_key&gt;\n</code></pre>"},{"location":"getting%20started/setup/#in-a-nutshell","title":"In a Nutshell","text":"<p>Your .env file (secrets in HuggingFace) should look like this:</p> <pre><code>CHAINLIT_AUTH_SECRET=&lt;your_jwt_secret&gt;\nOPENAI_API_KEY=&lt;your_openai_api_key&gt;\nHUGGINGFACE_TOKEN=&lt;your-huggingface-token&gt;\nLITERAL_API_KEY_LOGGING=&lt;your_api_key&gt;\nLITERAL_API_URL=&lt;https://cloud.getliteral.ai&gt;\nOAUTH_GOOGLE_CLIENT_ID=&lt;your_client_id&gt;\nOAUTH_GOOGLE_CLIENT_SECRET=&lt;your_client_secret&gt;\nLLAMA_CLOUD_API_KEY=&lt;your_api_key&gt;\nCHAINLIT_URL=&lt;your_chainlit_url&gt;\n</code></pre>"},{"location":"getting%20started/setup/#configuration","title":"Configuration","text":"<p>The configuration file <code>code/modules/config.yaml</code> contains the parameters that control the behaviour of your app. The configuration file <code>code/modules/user_config.yaml</code> contains user-defined parameters.</p>"},{"location":"roadmap/roadmap/","title":"Roadmap","text":""},{"location":"roadmap/roadmap/#in-the-pipeline","title":"In the Pipeline","text":"<ul> <li>Personalized Memory<ul> <li>\u23f3 Researching...</li> </ul> </li> <li>Agentic LLMs<ul> <li>\u23f3 Almost done...</li> </ul> </li> </ul>"},{"location":"roadmap/roadmap/#in-the-backlog","title":"In the Backlog","text":"<ul> <li>Chat Capabilities for the PDF Viewer<ul> <li>\ud83d\udca4 Waiting to be started...</li> </ul> </li> </ul>"}]}